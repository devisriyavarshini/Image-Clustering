# Image-Clustering
Image Clustering is an unsupervised machine learning technique that allows images to be clustered in groups based on similar visual properties and helps with a range of applications in content organization, retrieval, and anomaly detection. This work introduces a novel framework for image clustering using multimodal learning, leveraging the CLIP (Contrastive Languageâ€“Image Pretraining) model to combine image features with descriptive words. Traditional methods of image clustering based on unimodal algorithms often struggle with large datasets containing complex features. The proposed method bridges this gap by transforming unimodal image data into a multimodal framework through CLIP's descriptive capabilities. The use of text-based centroids that are optimized using k-means clustering algorithms has become the core of the method for enhancing cluster representation. These centroids introduce metadata-driven descriptions to the feature space, leading to a huge gain in performance. Experimental results confirm that clustering performance with these centroids approaches zero-shot distribution benchmarks, demonstrating the framework's robustness and adaptability across datasets such as CIFAR-10, STL10, and MIT Scene Recognition. While the method provides fast development and strong performance, scalability to large datasets is limited by clustering's computational complexity. This work promises much by vision-language models in unsupervised learning, with possibilities beyond traditional methods, and it opens the door to future innovation. Public resources supporting this methodology will encourage people to implement it even more. To address scalability and enhance feature quality, self-supervised pretraining methods such as SimCLR or BYOL can be integrated. These techniques offer robust feature representations, improving clustering accuracy and generalization, especially for complex datasets. Additionally, the LiT model can be used to enrich the image features further with semantic textual information, which provides richer and more context-aware representations. This multimodal approach improves the quality and interpretability of the learned features, especially in datasets involving both visual and text data.
